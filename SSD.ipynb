{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the annotation file into multiple .txt files for each image\n",
    "# Define the paths\n",
    "annotation_path = \"./grocery_annotations/annotation.txt\"\n",
    "image_dir = \"./Images_Grocery/ShelfImages/\"\n",
    "\n",
    "# Open the annotation file\n",
    "with open(annotation_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Process each line\n",
    "for line in lines:\n",
    "    parts = line.split()\n",
    "    image_name = parts[0]\n",
    "    num_objects = int(parts[1])\n",
    "    objects = [parts[i:i+5] for i in range(2, len(parts), 5)]\n",
    "\n",
    "    # Create a new annotation file for this image\n",
    "    with open(os.path.join(image_dir, image_name + '.txt'), 'w') as f:\n",
    "        for obj in objects:\n",
    "            # Write the object annotation to the file\n",
    "            f.write(' '.join(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def list_files(full_data_path = \"./Images_Grocery/ShelfImages/\", image_ext = '.JPG', test_size = 0.15, validation_size = 0.15):\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for r, d, f in os.walk(full_data_path):\n",
    "        for file in f:\n",
    "            if file.endswith(image_ext):\n",
    "                files.append(file)\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    train_files, test_files = train_test_split(files, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Split the training data into training and validation\n",
    "    train_files, val_files = train_test_split(train_files, test_size=validation_size, random_state=42)\n",
    "\n",
    "    return train_files, val_files, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files, validation_files, test_files = list_files()\n",
    "\n",
    "print(str(len(training_files)) + \" training files\")\n",
    "print(str(len(validation_files)) + \" validation files\")\n",
    "print(str(len(test_files)) + \" test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 244\n",
    "new_size = (244, 244)  # New size (height, width)\n",
    "\n",
    "def resize_image_and_box(img, box):\n",
    "    # Calculate the scale factors\n",
    "    y_scale = input_size / img.shape[0]\n",
    "    x_scale = input_size / img.shape[1]\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, new_size)\n",
    "\n",
    "    x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "    #new_box = [int(x / r), int(y / r), int(w / r), int(h / r)]\n",
    "    \n",
    "    # Scale the bounding box coordinates\n",
    "    resized_box = [int(box[0] * x_scale), int(box[1] * y_scale), int(box[2] * x_scale), int(box[3] * y_scale)]\n",
    "\n",
    "    return resized_img, resized_box\n",
    "\n",
    "def format_image(img1, box1):\n",
    "    img = img1.copy()\n",
    "    box = box1.copy()\n",
    "    height, width = img.shape\n",
    "    max_size = max(height, width)\n",
    "    r = max_size / input_size\n",
    "    new_width = int(width / r)\n",
    "    new_height = int(height / r)\n",
    "    new_size = (new_width, new_height)\n",
    "    resized = cv2.resize(img, new_size, interpolation= cv2.INTER_AREA)\n",
    "    new_image = np.zeros((input_size, input_size), dtype=np.uint8)\n",
    "    new_image[0:new_height, 0:new_width] = resized\n",
    "\n",
    "    x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "    new_box = [int(x / r), int(y / r), int(w / r), int(h / r)]\n",
    "    \n",
    "    return new_image, new_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_folder, transform=None):\n",
    "        self.dataset = self.create_dataset(images_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.dataset[idx]\n",
    "\n",
    "        # If you want to apply any image transformations\n",
    "        if self.transform:\n",
    "            entry[\"image\"] = self.transform(entry[\"image\"])\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        image = transforms.ToTensor()(entry[\"image\"])\n",
    "        #image = image / 255.0\n",
    "        # Convert bounding boxes and class labels to tensors\n",
    "        bounding_boxes = torch.tensor(entry[\"bounding_boxes\"], dtype=float)\n",
    "        class_labels = torch.tensor(entry[\"class_label\"], dtype=torch.int64)\n",
    "        bounding_boxes = bounding_boxes / torch.tensor([image.shape[2], image.shape[1], image.shape[2], image.shape[1]])\n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': bounding_boxes,\n",
    "            'labels': class_labels\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "    def create_dataset(self, files, full_data_path = \"./Images_Grocery/ShelfImages/\"):\n",
    "        dataset = []\n",
    "        \n",
    "        for file in files:\n",
    "            # Load the image in grayscale\n",
    "            img = cv2.imread(os.path.join(full_data_path, file), cv2.IMREAD_GRAYSCALE)\n",
    "            k = 1\n",
    "            labels = []\n",
    "            with open(os.path.join(full_data_path, file + \".txt\"), 'r') as fp:\n",
    "                lines = fp.readlines()\n",
    "                boxes = []\n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    box = np.array(parts[:4], dtype=int)\n",
    "                    xmin = box[0]\n",
    "                    ymin = box[1]\n",
    "                    xmax = box[0] + box[2]\n",
    "                    ymax = box[1] + box[3]\n",
    "                    box[0] = xmin\n",
    "                    box[1] = ymin\n",
    "                    box[2] = xmax\n",
    "                    box[3] = ymax\n",
    "                    if(box[0] == 0):\n",
    "                        box[0] = 0.1\n",
    "                    copy_img, box = format_image(img, box)\n",
    "                    #copy_img = copy_img.astype(int) / 255\n",
    "                    boxes.append(box)\n",
    "                    labels.append(k)\n",
    "                    \n",
    "                dataset.append({\n",
    "                    \"image\": copy_img,\n",
    "                    \"bounding_boxes\": boxes,\n",
    "                    \"class_label\": labels\n",
    "                })\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "custom_dataset = CustomDataset(training_files)\n",
    "validation_dataset = CustomDataset(validation_files)\n",
    "\n",
    "# Example: Using DataLoader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Get the first batch from the dataloader\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "# Get the first image, bounding boxes, and labels from the batch\n",
    "images = first_batch[0]\n",
    "targets = first_batch[1]\n",
    "#class_labels = first_batch['class_label']\n",
    "\n",
    "# Convert the image tensor to a numpy array and transpose the dimensions for matplotlib\n",
    "image = images[0].squeeze(0).squeeze(0)\n",
    "# Convert the image to float32\n",
    "image = image.numpy().astype(np.float32)\n",
    "\n",
    "temp_color_img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "image_height, image_width, _ = temp_color_img.shape\n",
    "\n",
    "boxes = targets['boxes'][0].numpy()\n",
    "labels = targets['labels'][0].numpy()\n",
    "\n",
    "for box, label in zip(boxes, labels):\n",
    "    norm_xmin, norm_ymin, norm_xmax, norm_ymax = box[0], box[1], box[2], box[3]\n",
    "    xmin = int(norm_xmin * image_width)\n",
    "    ymin = int(norm_ymin * image_height)\n",
    "    xmax = int(norm_xmax * image_width)\n",
    "    ymax = int(norm_ymax * image_height)\n",
    "\n",
    "    #cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "    cv2.rectangle(temp_color_img, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "    plt.text(xmin, ymin, str(label), color='red')\n",
    "plt.imshow(temp_color_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PredictionNet(nn.Module):\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        super(PredictionNet, self).__init__()\n",
    "        num_boxes = {\"conv4_3\": 4, \"conv7\": 6, \"conv8_2\": 6,\n",
    "                     \"conv9_2\": 6, \"conv10_2\": 4, \"conv11_2\": 4}\n",
    "\n",
    "        self.loc_layers = nn.ModuleList()\n",
    "        self.cls_layers = nn.ModuleList()\n",
    "        self.num_classes = num_classes\n",
    "        in_channels = {\"conv4_3\": 512, \"conv7\":512 , \"conv8_2\": 512, \"conv9_2\": 256, \"conv10_2\": 256, \"conv11_2\": 256}\n",
    "        \n",
    "        for feature_map in [\"conv4_3\", \"conv7\", \"conv8_2\", \"conv9_2\", \"conv10_2\", \"conv11_2\"]:\n",
    "            loc_layer = nn.Conv2d(in_channels[feature_map], num_boxes[feature_map] * 4, kernel_size=(3, 3), padding=1)\n",
    "            cls_layer = nn.Conv2d(in_channels[feature_map], num_boxes[feature_map] * num_classes, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "            self.loc_layers.append(loc_layer)\n",
    "            self.cls_layers.append(cls_layer)\n",
    "        self.weights_init()\n",
    "\n",
    "    def forward(self, out_vgg, out_conv7, out_conv8, out_conv9, out_conv10, out_conv11):\n",
    "        # Location predictions\n",
    "        batch_size = out_vgg.size(0)\n",
    "        locs_pred = []\n",
    "        for i, conv_out in enumerate([out_vgg, out_conv7, out_conv8, out_conv9, out_conv10, out_conv11]):\n",
    "            locs_pred.append(self.loc_layers[i](conv_out).permute(0, 2, 3, 1).contiguous().view(conv_out.size(0), -1, 4))\n",
    "        \n",
    "        locs_pred = torch.cat(locs_pred, dim=1)\n",
    "\n",
    "        # Class predictions\n",
    "        cls_pred = []\n",
    "        for i, conv_out in enumerate([out_vgg, out_conv7, out_conv8, out_conv9, out_conv10, out_conv11]):\n",
    "            cls_pred.append(self.cls_layers[i](conv_out).permute(0, 2, 3, 1).contiguous().view(conv_out.size(0), -1, self.num_classes))\n",
    "        \n",
    "        cls_pred = torch.cat(cls_pred, dim=1)\n",
    "\n",
    "        return locs_pred, cls_pred\n",
    "\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for loc_layer, cls_layer in zip(self.loc_layers, self.cls_layers):\n",
    "            nn.init.xavier_uniform_(loc_layer.weight)\n",
    "            nn.init.zeros_(loc_layer.bias)\n",
    "            nn.init.xavier_uniform_(cls_layer.weight)\n",
    "            nn.init.zeros_(cls_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet (nn.Module):\n",
    "    def __init__(self ) -> None:\n",
    "        super(ConvNet,self).__init__()\n",
    "        self.conv8_1 = nn.Conv2d(512, 256, kernel_size=(1, 1), padding= 0)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size= (3, 3), padding= 1, stride= 2)\n",
    "        \n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size= (1, 1), padding= 0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size= (3, 3), padding= 1, stride= 2)\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size= (1, 1), padding= 0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size= (3, 3), padding= 0)\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size= (1, 1), padding= 0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size= (3, 3), padding= 0) \n",
    "    def forward(self, conv7_out):\n",
    "       \n",
    "        x = conv7_out    #(N, 512, 19, 19)\n",
    "        x = F.relu(self.conv8_1(x))    #(N, 256, 19, 19)\n",
    "        x = F.relu(self.conv8_2(x))    #(N, 512, 10, 10)\n",
    "        conv8_2_out = x\n",
    "        \n",
    "        x = F.relu(self.conv9_1(x))    #(N, 128, 10, 10)\n",
    "        x = F.relu(self.conv9_2(x))    #(N, 256, 5, 5)\n",
    "        conv9_2_out = x\n",
    "        \n",
    "        x = F.relu(self.conv10_1(x))   #(N, 128, 5, 5)\n",
    "        x = F.relu(self.conv10_2(x))   #(N, 256, 3, 3)\n",
    "        conv10_2_out = x\n",
    "        \n",
    "        x = F.relu(self.conv11_1(x))   #(N, 128, 3, 3)\n",
    "        conv11_2_out = F.relu(self.conv11_2(x))   #(N, 256, 1, 1)\n",
    "\n",
    "        return conv8_2_out, conv9_2_out, conv10_2_out, conv11_2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    def __init__(self, default_boxes, threshold=0.5, neg_pos=3, alpha=1):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.default_boxes=default_boxes\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos = neg_pos\n",
    "        self.alpha = alpha\n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    def forward(self, locs_pred, cls_pred, boxes, labels):\n",
    "        batch_size = locs_pred.size(0)\n",
    "        n_default_boxes = self.default_boxes.size(0)\n",
    "        num_classes = cls_pred.size(2)\n",
    "\n",
    "\n",
    "        # Initialize tensors for target values\n",
    "        true_locs = torch.zeros((batch_size, n_default_boxes, 4), dtype=torch.float).to(locs_pred.device)\n",
    "        true_classes = torch.zeros((batch_size, n_default_boxes), dtype=torch.long).to(locs_pred.device)\n",
    "\n",
    "        # Target assignment\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0)\n",
    "\n",
    "            overlap = self.IoU(boxes[i], self.default_boxes)\n",
    "\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)\n",
    "\n",
    "            _, prior_for_each_object = overlap.max(dim=1)\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(locs_pred.device)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0\n",
    "\n",
    "            true_classes[i] = label_for_each_prior\n",
    "            true_locs[i] = boxes[i][object_for_each_prior], self.default_boxes\n",
    "\n",
    "\n",
    "        # Compute localization loss\n",
    "        pos = true_classes > 0\n",
    "        loc_loss = F.smooth_l1_loss(locs_pred[pos], true_locs[pos])\n",
    "\n",
    "        # Compute confidence loss\n",
    "        conf_loss = F.cross_entropy(cls_pred.view(-1, num_classes), true_classes.view(-1), ignore_index=-1)\n",
    "\n",
    "        return loc_loss, conf_loss\n",
    "\n",
    "    def IoU(self, box_a, box_b):\n",
    "        inter = self.intersect(box_a, box_b)\n",
    "        area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])\n",
    "        area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])\n",
    "        union = area_a.unsqueeze(1) + area_b.unsqueeze(0) - inter\n",
    "        return inter / union\n",
    "\n",
    "    def intersect(self, box_a, box_b):\n",
    "        max_xy = torch.min(box_a[:, 2:].unsqueeze(1), box_b[:, 2:].unsqueeze(0))\n",
    "        min_xy = torch.max(box_a[:, :2].unsqueeze(1), box_b[:, :2].unsqueeze(0))\n",
    "        inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "        return inter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import math\n",
    "import torch\n",
    "class ModifiedVGG16(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(ModifiedVGG16, self).__init__()\n",
    "        \n",
    "        # Ladda ner hela VGG-16-modellen\n",
    "        vgg16 = models.vgg16(pretrained=pretrained)\n",
    "        print(vgg16)\n",
    "        # Hämta lager från VGG-16\n",
    "        self.features = vgg16.features\n",
    "        \n",
    "        # Lägg till nya lager efter VGG-16\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "        self.conv7 = nn.Conv2d(1024, 512, kernel_size=1)\n",
    "        \n",
    "        # Byt ut sista fully connected layer för din specifika uppgift\n",
    "        vgg16.classifier[6] = nn.Linear(4096, 1024)\n",
    "        \n",
    "        # Kopiera över de modifierade lagren till din modell\n",
    "        self.load_state_dict(vgg16.state_dict(), strict=False)\n",
    "           # Lägg till AuxiliaryNet och PredictionNet\n",
    "        self.auxiliary_net = ConvNet()\n",
    "        self.prediction_net = PredictionNet(num_classes=num_classes)\n",
    "\n",
    "        # Lägg till default_boxes som en attribut\n",
    "        default_boxes=self.create_default_boxes(num_channels=1)\n",
    "        # Skapa en instans av MultiBoxLoss med default_boxes\n",
    "        self.multibox_loss = MultiBoxLoss(default_boxes=default_boxes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        vgg=x \n",
    "       \n",
    "        x = F.interpolate(x, size=(19, 19), mode='bilinear', align_corners=False)\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x =F.relu( self.conv7(x))\n",
    "        conv7=x\n",
    "         # Lägg till anrop för ConvNet\n",
    "        aux_out = self.auxiliary_net(conv7)\n",
    "        print(len(aux_out))\n",
    "        locs_pred, cls_pred = self.prediction_net.forward(vgg, conv7, aux_out[0], aux_out[1], aux_out[2], aux_out[3])\n",
    "        \n",
    "        # Beräkna förlusten med MultiBoxLoss\n",
    "        loss = self.multibox_loss(locs_pred, cls_pred, targets['boxes'], targets['labels'])\n",
    "\n",
    "        return loss, aux_out, locs_pred, cls_pred\n",
    "    \n",
    "\n",
    "    def create_default_boxes(self, num_channels):\n",
    "        fmap_wh = {\"vgg\": 38, \"conv7\": 19, \"conv8\": 10, \"conv9\": 5,\n",
    "                \"conv10\": 3, \"conv11\": 1}\n",
    "\n",
    "        scales = {\"vgg\": 0.1, \"conv7\": 0.2, \"conv8\": 0.375,\n",
    "                \"conv9\": 0.55, \"conv10\": 0.725, \"conv11\": 0.9}\n",
    "\n",
    "        aspect_ratios = {\"vgg\": [1., 2., 0.5], \"conv7\": [1., 2., 3., 0.5, 0.3333],\n",
    "                        \"conv8\": [1., 2., 3., 0.5, 0.3333],\n",
    "                        \"conv9\": [1., 2., 3., 0.5, 0.3333],\n",
    "                        \"conv10\": [1., 2., 0.5], \"conv11\": [1., 2., 0.5]}\n",
    "\n",
    "        fmaps = list(fmap_wh.keys())\n",
    "\n",
    "        default_boxes = []\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_wh[fmap]):\n",
    "                for j in range(fmap_wh[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_wh[fmap]\n",
    "                    cy = (i + 0.5) / fmap_wh[fmap]\n",
    "\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        default_boxes.append([cx, cy, scales[fmap] * math.sqrt(ratio),\n",
    "                                            scales[fmap] / math.sqrt(ratio)])  # (cx, cy, w, h)\n",
    "\n",
    "                        if ratio == 1:\n",
    "                            try:\n",
    "                                add_scale = math.sqrt(scales[fmap] * scales[fmaps[k + 1]])\n",
    "                            except IndexError:\n",
    "                                # for the last feature map\n",
    "                                add_scale = 1.\n",
    "                            default_boxes.append([cx, cy, add_scale, add_scale])\n",
    "\n",
    "        default_boxes = torch.FloatTensor(default_boxes).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) # (8732, 4)\n",
    "        default_boxes.clamp_(0, 1)\n",
    "        return default_boxes\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Skapa en instans av din modifierade VGG-16-modell\n",
    "num_classes = 2  # Ersätt med det faktiska antalet klasser i din uppgift\n",
    "ssd_model= ModifiedVGG16(num_classes=num_classes, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 1, 244, 244] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m targets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m targets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Gör förutsägelser\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m loss, _, locs_pred, cls_pred \u001b[38;5;241m=\u001b[39m ssd_model(images)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Beräkna förlusten och uppdatera vikterna\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 33\u001b[0m, in \u001b[0;36mModifiedVGG16.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m     34\u001b[0m     vgg\u001b[38;5;241m=\u001b[39mx \n\u001b[0;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m19\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 1, 244, 244] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "from torch import device\n",
    "from torch.optim import Adam\n",
    "# Optimizer\n",
    "optimizer = Adam(ssd_model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss function\n",
    "multibox_loss = MultiBoxLoss(default_boxes=ssd_model.create_default_boxes(1))\n",
    "# Träningsloopen\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ssd_model.train()\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ladda data till enheten\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        images = images.to(device)\n",
    "        targets['boxes'] = targets['boxes'].to(device)\n",
    "        targets['labels'] = targets['labels'].to(device)\n",
    "\n",
    "        # Gör förutsägelser\n",
    "        loss, _, locs_pred, cls_pred = ssd_model(images)\n",
    "\n",
    "        # Beräkna förlusten och uppdatera vikterna\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Skriv ut framsteg\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item()}')\n",
    "\n",
    "# Sparar modellen efter träning\n",
    "torch.save(ssd_model.state_dict(), 'ssd_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Skapa en enkel datasetklass för teständamål\n",
    "# class TestDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, num_samples=100, input_size=(3, 244, 244)):\n",
    "#         self.num_samples = num_samples\n",
    "#         self.input_size = input_size\n",
    "#         self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Skapa en slumpmässig bild med rätt indatastorlek (3, 300, 300)\n",
    "#         sample = torch.rand(self.input_size)\n",
    "#         return sample\n",
    "\n",
    "# # Skapa testdataset och dataloaderA\n",
    "# test_dataset = TestDataset()\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# # Flytta modellen till önskad enhet (CPU eller GPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ssd_model.to(device)\n",
    "\n",
    "# # Testa modellen\n",
    "# ssd_model.eval()\n",
    "# for batch_idx, inputs in enumerate(test_loader):\n",
    "#     inputs = inputs.to(device)\n",
    "#     print(inputs.shape)  # Lägg till denna rad för att skriva ut storleken på dina inputs\n",
    "#     outputs = ssd_model(inputs)\n",
    "\n",
    "#     # Anta att cls_pred är ditt output för klassprediktionen\n",
    "#     cls_pred = outputs[-1]\n",
    "\n",
    "#     # Du kan utföra ytterligare utvärderingar eller inspektioner här baserat på ditt behov\n",
    "#     print(f\"Batch {batch_idx}, Output Shape: {cls_pred.shape}\")\n",
    "\n",
    "# # Kom ihåg att återgå modellen till träningsläge om den behöver träna igen\n",
    "# ssd_model.train()\n",
    "\n",
    "\n",
    "# # # Flytta modellen till önskad enhet (CPU eller GPU)\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # ssd_model.to(device)\n",
    "\n",
    "# # # Välj en optimeringsalgoritm och inställningar\n",
    "# # optimizer = optim.Adam(ssd_model.parameters(), lr=0.001)\n",
    "\n",
    "# # # Loop för träningssteg\n",
    "# # for epoch in range(num_epochs):\n",
    "# #     for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "# #         # Nollställ gradienter\n",
    "# #         optimizer.zero_grad()\n",
    "\n",
    "# #         # Gör förutsägelser och beräkna förlust\n",
    "# #         outputs = ssd_model(inputs)\n",
    "# #         loss = calculate_multibox_loss(outputs, targets)  # Implementera eller använd en befintlig funktion för att beräkna förlusten\n",
    "\n",
    "# #         # Backpropagation och optimering\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "\n",
    "# #         # Utvärdera och/eller skriv ut resultaten efter varje träningssteg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
